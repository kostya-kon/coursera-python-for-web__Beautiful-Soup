# coursera-python-for-web__Beautiful-Soup
## BeautifulSoup task from https://www.coursera.org/learn/python-for-web course
--------------------------------------------------
### Описание выполненого задания
--------------------------------------------------
## Часть 1:
В этом задании вам необходимо реализовать парсер для сбора статистики со страниц Википедии. Чтобы упростить вашу задачу, необходимые страницы уже скачаны и сохранены на файловой системе в директории wiki/ (Например, страница https://en.wikipedia.org/wiki/Stone_Age сохранена файле wiki/Stone_Age). 
Парсер реализован в виде функции parse, которая принимает на вход один параметр: path_to_file — путь до файла, содержащий html код страницы википедии. Гарантируется, что такой путь существует. Ваша задача — прочитать файл, пройтись Beautiful Soup по статье, найти её тело (это`<div id="bodyContent">`) и внутри него подсчитать:

* Количество картинок (img) с шириной (width) не меньше 200. Например: `<img width="200">`, но не `<img>` и не `<img width="199">`
* Количество заголовков (`h1`, `h2`, `h3`, `h4`, `h5`, `h6`), первая буква текста внутри которых соответствует заглавной букве E, T или C. Например: `<h1>End</h1>` или `<h5><span>Contents</span></h5>`, но не `<h1>About</h1>` и не `<h2>end</h2>` и не `<h3><span>1</span><span>End</span></h3>`
* Длину максимальной последовательности ссылок, между которыми нет других тегов, открывающихся или закрывающихся. Например: `<p><span><a></a></span>`, `<a></a>`, `<a></a></p>` - тут 2 ссылки подряд, т.к. закрывающийся span прерывает последовательность. `<p><a><span></span></a>`, `<a></a>`, `<a></a></p>` - а тут 3 ссылки подряд, т.к. span находится внутри ссылки, а не между ссылками.
* Количество списков (`ul`, `ol`), не вложенных в другие списки. Например: `<ol><li></li></ol>`, `<ul><li><ol><li></li></ol></li></ul>` - два не вложенных списка (и один вложенный)

Результатом работы функции parse будет список четырех чисел, посчитанных по формулам выше.
Так же, не упустите момент, что данные во всех пунктах нужно искать внутри `<div id="bodyContent">`, а не по всей странице.

**Пример работы функции parse:**
```
>>> parse("wiki/Stone_Age")

>>> [13, 10, 12, 40]
```

## Часть 2:

В этом задании продолжаем работать со страницами из wikipedia. Необходимо реализовать механизм сбора статистики по нескольким страницам. Сложность задачи состоит в том, что сначала нужно будет определить страницы, с которых необходимо собирать статистику. В качестве входных данных служат названия двух статей(страниц). Гарантируется, что файлы обеих статей есть в папке wiki и из первой статьи можно попасть в последнюю, переходя по ссылкам только на те статьи, копии которых есть в папке wiki.

Например, на вход подаются страницы: Stone_Age и Python_(programming_language). В статье Stone_Age есть ссылка на Brain, в ней на Artificial_intelligence, а в ней на Python_(programming_language) и это кратчайший путь от Stone_Age до Python_(programming_language). Ваша задача — найти самый короткий путь (гарантируется, что существует только один путь минимальной длины), а затем с помощью функции parse из предыдущего задания собрать статистику по всем статьям в найденном пути. 

Результат нужно вернуть в виде словаря, ключами которого являются имена статей, а значениями списки со статистикой. Для нашего примера правильный результат будет :
```
{ 'Stone_Age': [13, 10, 12, 40], 
  'Brain': [19, 5, 25, 11], 
  'Artificial_intelligence': [8, 19, 13, 198], 
  'Python_(programming_language)': [2, 5, 17, 41] 
}
```
Вам необходимо реализовать две функции:  **build_bridge и get_statistics**. 

Обе функции принимают на вход три параметра:__path__ - путь до директории с сохраненными файлами из wikipedia,__start_page__ - название начальной страницы,__end_page__ - название конечной страницы.

Функция build_bridge вычисляет кратчайший путь и возвращает список страниц в том порядке, в котором происходят переходы. Начальная и конечная страницы включаются в результирующий список. В случае, если название стартовой и конечной страницы совпадают, то результирующий список должен содержать только стартовую страницу. Получить все ссылки на странице можно разными способами, в том числе и с помощью регулярных выражений, например так: 
```
with open(os.path.join(path, page), encoding="utf-8") as file:
    links = re.findall(r"(?<=/wiki/)[\w()]+", file.read())
```

Обратите внимание, что что на страницах wikipedia могут встречаться ссылки на страницы, которых нет в директории wiki, такие ссылки должны игнорироваться. 

__Пример работы функции build_bridge:__
```
>>> result = build_bridge('wiki/', 'The_New_York_Times', 'Stone_Age')
>>> print(result)
['The_New_York_Times', 'London', 'Woolwich', 'Iron_Age', 'Stone_Age']
```
Функция get_statistics использует функцию parse и собирает статистику по страницам, найденным с помощью функции build_bridge.

__Пример работы функции get_statistics:__
```
>>> from pprint import pprint
>>> result = get_statistics('wiki/', 'The_New_York_Times', "Binyamina_train_station_suicide_bombing")
>>> pprint(result)
{'Binyamina_train_station_suicide_bombing': [1, 3, 6, 21],
 'Haifa_bus_16_suicide_bombing': [1, 4, 15, 23],
 'Second_Intifada': [9, 13, 14, 84],
 'The_New_York_Times': [5, 9, 8, 42]}
 ```
